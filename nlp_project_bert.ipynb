{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_project_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "FqwncoEVPHcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E_26muz1vFe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import os\n",
        "\n",
        "base_directory = 'nlp/stanfordSentimentTreebank/'\n",
        "sentences = pd.read_csv('nlp/stanfordSentimentTreebank/datasetSentences.txt', index_col=\"sentence_index\",\n",
        "                                sep=\"\\t\")\n",
        "splits = pd.read_csv(os.path.join(base_directory, \"datasetSplit.txt\"), index_col=\"sentence_index\")\n",
        "sentences = sentences.join(splits)\n",
        "sentences = sentences.sort_values(by=['splitset_label', 'sentence_index'])\n",
        "sentences.reset_index(inplace=True)\n",
        "\n",
        "sents_df = pd.DataFrame()\n",
        "\n",
        "for i,data in enumerate(['nlp/trees/train.txt', 'nlp/trees/test.txt', 'nlp/trees/dev.txt']):\n",
        "  new = pd.read_csv(data, sep=\"\\t\", header=None)\n",
        "  new = new.apply(lambda it: str(it).split()[1][1], axis=1).to_frame().rename(columns={0: 'label'})\n",
        "  new['set'] = i+1\n",
        "  sents_df = sents_df.append(new)\n",
        "\n",
        "sents_df.reset_index(inplace=True, drop=True)\n",
        "sents_df = pd.concat([sentences, sents_df], axis=1)\n",
        "sents_df.label = sents_df.label.astype(float)\n",
        "sents_df.to_pickle('nlp/sents.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "phrases = pd.read_csv('nlp/stanfordSentimentTreebank/dictionary.txt', header=None,\n",
        "                                sep=\"|\").rename(columns={0: 'sentence', 1: 'phrase_id'})\n",
        "phrase_sentiments = pd.read_csv('nlp/stanfordSentimentTreebank/sentiment_labels.txt',\n",
        "                                sep=\"|\").rename(columns={'phrase ids': 'phrase_id', 'sentiment values': 'label'})\n",
        "phrase_df = pd.merge(phrases, phrase_sentiments, on='phrase_id')\n",
        "phrase_df.to_pickle('nlp/phrases.pkl')"
      ],
      "metadata": {
        "id": "QUksZpAaegc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "phrase_df = pd.read_pickle('nlp/phrases.pkl')\n",
        "sents_df = pd.read_pickle('nlp/sents.pkl')\n",
        "\n",
        "def factorize(value):\n",
        "  if value < 0.2:\n",
        "    return 0\n",
        "  elif 0.2 <= value < 0.4:\n",
        "    return 1\n",
        "  elif 0.4<= value < 0.6:\n",
        "    return 2\n",
        "  elif 0.6<= value < 0.8:\n",
        "    return 3\n",
        "  elif 0.8<= value:\n",
        "    return 4\n",
        "\n",
        "phrase_df['label'] = phrase_df['label'].apply(factorize)"
      ],
      "metadata": {
        "id": "rwYq27JkR3WT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "def predict(model, tokenizer, sentence, max_len):\n",
        "    labels = {0: 'very negative', 1: 'negative', 2: 'neutral', 3: 'positive', 4: 'very positive'}\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    input = tokenizer.encode_plus(\n",
        "      sentence,\n",
        "      add_special_tokens=True,\n",
        "      max_length=max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "      values = []\n",
        "      input_ids = input[\"input_ids\"].to(device)\n",
        "      attention_mask = input[\"attention_mask\"].to(device)\n",
        "\n",
        "      outputs = model(input_ids, attention_mask)\n",
        "      preds = outputs.logits.argmax().item()\n",
        "\n",
        "      return labels[preds]\n",
        "\n",
        "def evaluate(model, test_set):\n",
        "  test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=128)\n",
        "\n",
        "  ## Evaluate performance on validation set\n",
        "  y_pred = []\n",
        "  y_test = []\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    loss_counter = 0\n",
        "    for batch, d in enumerate(test_dataloader):\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"labels\"].argmax(-1).cpu()\n",
        "\n",
        "      outputs = model(input_ids, attention_mask)\n",
        "      preds = outputs.logits.argmax(-1).cpu()\n",
        "\n",
        "      y_pred.extend(preds)\n",
        "      y_test.extend(targets)\n",
        "\n",
        "\n",
        "  print(y_pred, y_test)\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  precision, recall, fscore, support  = precision_recall_fscore_support(y_test, y_pred)\n",
        "  print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
        "  print(\"f1: {}\".format(f1_score(y_test, y_pred, average='weighted')))\n",
        "  print('precision: {}'.format(precision))\n",
        "  print('recall: {}'.format(recall))\n",
        "  print('fscore: {}'.format(fscore))\n",
        "  print('support: {}'.format(support))"
      ],
      "metadata": {
        "id": "mNXYHHFWkGtx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, dataset, tokenizer) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    df = dataset.reset_index()\n",
        "    self.labels = list(LabelBinarizer().fit_transform(df['label'].round(0)))\n",
        "    sents = df['sentence'].to_list()\n",
        "\n",
        "    self.encodings = tokenizer(sents, truncation=True, padding=True)\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[index], dtype=torch.float)\n",
        "\n",
        "    return item\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    return output[0]"
      ],
      "metadata": {
        "id": "71D_mSPX70D7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Manual training with pyTorch\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def train_model(model, train_set, validation_set, max_epochs, batch_size):\n",
        "\n",
        "  evaluation_data = {\n",
        "      'train_loss': [],\n",
        "      'validation_loss': [],\n",
        "  }\n",
        "\n",
        "  # Define model and loss functions\n",
        "  model = model.to(device)\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "  criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "  # Define dataloaders\n",
        "  dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "  validation_dataloader = torch.utils.data.DataLoader(validation_set, batch_size=64)\n",
        "\n",
        "  # Train loop\n",
        "  for epoch in range(max_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "\n",
        "    ## Train in batches\n",
        "    epoch_loss = 0\n",
        "    loss_counter = 0\n",
        "    for batch, d in enumerate(dataloader):\n",
        "      if batch%100 == 0:\n",
        "        print(f'Epoch {epoch} - Batch {batch} of {int(len(train_set)/batch_size)}')\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"labels\"].to(device)\n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = criterion(outputs, targets)\n",
        "\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      loss_counter +=1\n",
        "\n",
        "    epoch_loss /= loss_counter\n",
        "    \n",
        "\n",
        "    ## Evaluate performance on validation set\n",
        "    print(f'Epoch {epoch} - Evaluating validation set')\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      valid_loss = 0\n",
        "      loss_counter = 0\n",
        "      for batch, d in enumerate(validation_dataloader):\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        valid_loss += loss.item()\n",
        "        loss_counter +=1\n",
        "\n",
        "      valid_loss /= loss_counter\n",
        "\n",
        "    ## update evaluation data\n",
        "    evaluation_data['train_loss'].append(epoch_loss)\n",
        "    evaluation_data['validation_loss'].append(valid_loss)\n",
        "\n",
        "    print(\"Train Epoch {}: Time {}s |  Loss - {} | Validation loss - {}\".format(epoch, int(time.time() - start_time), epoch_loss, valid_loss))\n",
        "    print(\"----------------------------------------\")\n",
        "  return evaluation_data\n"
      ],
      "metadata": {
        "id": "zURNZQEffsQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train with Trainer\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids.argmax(-1)\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "\n",
        "  return {\n",
        "      'accuracy': accuracy_score(labels, preds),\n",
        "      'f1': f1_score(labels, preds, average='weighted')\n",
        "  }\n",
        "\n",
        "\n",
        "device = \"cuda\"\n",
        "distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True, model_max_length=512, batched=True)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5, problem_type=\"multi_label_classification\").to(device)\n",
        "\n",
        "\n",
        "df = phrase_df.sample(frac=1)\n",
        "phrase_train_dataset, phrase_test_dataset = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "phrase_test_dataset, phrase_validation_dataset = train_test_split(phrase_test_dataset, test_size=0.5)\n",
        "\n",
        "phrase_train_dataset = SentimentDataset(phrase_train_dataset, distilbert_tokenizer)\n",
        "phrase_validation_dataset = SentimentDataset(phrase_validation_dataset, distilbert_tokenizer)\n",
        "phrase_test_dataset = SentimentDataset(phrase_test_dataset, distilbert_tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    seed=0,\n",
        "    output_dir='nlp/bert/phrases/results',          # output directory\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='nlp/bert/phrases/logs',            # directory for storing logs\n",
        "    logging_steps=500,\n",
        "    no_cuda=False,\n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy=\"steps\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=phrase_train_dataset,         # training dataset\n",
        "    eval_dataset=phrase_validation_dataset,             # evaluation dataset\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "eHwkVX1KUBpH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16675598-9fe6-464a-bc49-ed6010255663"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"problem_type\": \"multi_label_classification\",\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "using `logging_steps` to initialize `eval_steps` to 500\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 191385\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 11962\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11962' max='11962' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11962/11962 1:49:13, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.393900</td>\n",
              "      <td>0.304649</td>\n",
              "      <td>0.638355</td>\n",
              "      <td>0.617146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.303100</td>\n",
              "      <td>0.297260</td>\n",
              "      <td>0.642117</td>\n",
              "      <td>0.630199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.294600</td>\n",
              "      <td>0.281509</td>\n",
              "      <td>0.665817</td>\n",
              "      <td>0.661126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.291900</td>\n",
              "      <td>0.280302</td>\n",
              "      <td>0.667614</td>\n",
              "      <td>0.668088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.286700</td>\n",
              "      <td>0.278043</td>\n",
              "      <td>0.671836</td>\n",
              "      <td>0.651935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.277600</td>\n",
              "      <td>0.279748</td>\n",
              "      <td>0.669119</td>\n",
              "      <td>0.670899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.275200</td>\n",
              "      <td>0.272573</td>\n",
              "      <td>0.680572</td>\n",
              "      <td>0.680049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.275500</td>\n",
              "      <td>0.266595</td>\n",
              "      <td>0.685253</td>\n",
              "      <td>0.683794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.268700</td>\n",
              "      <td>0.264857</td>\n",
              "      <td>0.691690</td>\n",
              "      <td>0.684320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.271900</td>\n",
              "      <td>0.262613</td>\n",
              "      <td>0.693989</td>\n",
              "      <td>0.691101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.266500</td>\n",
              "      <td>0.264349</td>\n",
              "      <td>0.689433</td>\n",
              "      <td>0.688073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.265900</td>\n",
              "      <td>0.258702</td>\n",
              "      <td>0.702098</td>\n",
              "      <td>0.696358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.234700</td>\n",
              "      <td>0.263310</td>\n",
              "      <td>0.696455</td>\n",
              "      <td>0.694867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.236100</td>\n",
              "      <td>0.268074</td>\n",
              "      <td>0.688890</td>\n",
              "      <td>0.686368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.236700</td>\n",
              "      <td>0.267484</td>\n",
              "      <td>0.695118</td>\n",
              "      <td>0.691574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.234800</td>\n",
              "      <td>0.270485</td>\n",
              "      <td>0.688973</td>\n",
              "      <td>0.689681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.238900</td>\n",
              "      <td>0.261554</td>\n",
              "      <td>0.699214</td>\n",
              "      <td>0.698616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.233200</td>\n",
              "      <td>0.260589</td>\n",
              "      <td>0.697375</td>\n",
              "      <td>0.696342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.233900</td>\n",
              "      <td>0.259228</td>\n",
              "      <td>0.700552</td>\n",
              "      <td>0.699467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.260269</td>\n",
              "      <td>0.704941</td>\n",
              "      <td>0.702054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.228700</td>\n",
              "      <td>0.259033</td>\n",
              "      <td>0.701931</td>\n",
              "      <td>0.699754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.231800</td>\n",
              "      <td>0.257835</td>\n",
              "      <td>0.705066</td>\n",
              "      <td>0.704123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.229200</td>\n",
              "      <td>0.258134</td>\n",
              "      <td>0.702851</td>\n",
              "      <td>0.702506</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-1000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-1000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-1500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-1500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-1500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-2000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-2000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-2000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-2500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-2500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-2500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-3000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-3000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-3000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-3500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-3500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-3500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-4000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-4000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-4000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-4500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-4500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-4500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-5000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-5000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-5000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-5500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-5500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-5500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-6000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-6000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-6000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-6500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-6500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-6500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-7000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-7000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-7000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-7500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-7500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-7500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-8000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-8000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-8000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-8500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-8500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-8500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-9000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-9000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-9000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-9500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-9500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-9500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-10000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-10000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-10000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-10500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-10500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-10500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-11000\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-11000/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-11000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 23924\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to nlp/bert/phrases/results/checkpoint-11500\n",
            "Configuration saved in nlp/bert/phrases/results/checkpoint-11500/config.json\n",
            "Model weights saved in nlp/bert/phrases/results/checkpoint-11500/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from nlp/bert/phrases/results/checkpoint-11000 (score: 0.2578350603580475).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=11962, training_loss=0.26136551515610795, metrics={'train_runtime': 6554.3315, 'train_samples_per_second': 58.4, 'train_steps_per_second': 1.825, 'total_flos': 6536482594428600.0, 'train_loss': 0.26136551515610795, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = sents_df[sents_df.splitset_label == 1]\n",
        "test = sents_df[sents_df.splitset_label == 2]\n",
        "dev = sents_df[sents_df.splitset_label == 3]\n",
        "\n",
        "sents_train_dataset = SentimentDataset(train, distilbert_tokenizer)\n",
        "sents_validation_dataset = SentimentDataset(dev, distilbert_tokenizer)\n",
        "sents_test_dataset = SentimentDataset(test, distilbert_tokenizer)\n",
        "\n",
        "trainer.predict(phrase_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "xeALbOemHuMQ",
        "outputId": "4cdee32a-14cc-43ac-cb7c-d49f37d51074"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 2392\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='73' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:53]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-5.3413424 , -3.4993482 , -1.093116  ,  0.7655581 , -3.1847749 ],\n",
              "       [-6.4875865 , -4.6604743 ,  2.3212569 , -2.3729525 , -5.8557615 ],\n",
              "       [-2.3275664 , -0.7624236 , -1.0487303 , -1.5854385 , -3.6403081 ],\n",
              "       ...,\n",
              "       [-4.030131  , -0.7505966 ,  0.33073053, -3.065243  , -5.710458  ],\n",
              "       [-6.044316  , -4.6209316 ,  3.121001  , -3.2444077 , -5.768075  ],\n",
              "       [-1.1600947 ,  0.29896525, -2.2655993 , -4.1514773 , -5.298276  ]],\n",
              "      dtype=float32), label_ids=array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.2983350455760956, 'test_accuracy': 0.6500836120401338, 'test_f1': 0.6450424637728529, 'test_runtime': 8.2458, 'test_samples_per_second': 290.086, 'test_steps_per_second': 4.608})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments\n",
        "device = \"cuda\"\n",
        "distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True, model_max_length=512, batched=True)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"nlp/bert/phrases/results/checkpoint-11000\", num_labels=5, problem_type=\"multi_label_classification\").to(device)\n",
        "\n",
        "train = sents_df[sents_df.splitset_label == 1]\n",
        "test = sents_df[sents_df.splitset_label == 2]\n",
        "dev = sents_df[sents_df.splitset_label == 3]\n",
        "\n",
        "sents_train_dataset = SentimentDataset(train, distilbert_tokenizer)\n",
        "sents_validation_dataset = SentimentDataset(dev, distilbert_tokenizer)\n",
        "sents_test_dataset = SentimentDataset(test, distilbert_tokenizer)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids.argmax(-1)\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "\n",
        "  return {\n",
        "      'accuracy': accuracy_score(labels, preds),\n",
        "      'f1': f1_score(labels, preds, average='weighted'),\n",
        "      'confusion': str(confusion_matrix(labels, preds))\n",
        "  }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.predict(sents_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "EUgVIqdax3aY",
        "outputId": "99b77e1e-e8f8-40e6-8474-faa9b61aa755"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "***** Running Prediction *****\n",
            "  Num examples = 2210\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='277' max='277' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [277/277 00:11]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-5.8134    , -0.76526517,  0.3534432 , -2.4080489 , -7.5354104 ],\n",
              "       [-8.696852  , -5.9402914 , -2.24961   ,  1.4550132 , -2.2659087 ],\n",
              "       [-9.246331  , -7.2609954 , -4.455598  , -0.42864442,  0.35292432],\n",
              "       ...,\n",
              "       [-9.368487  , -7.0722756 , -3.8692892 ,  0.14449883, -0.25039145],\n",
              "       [-3.8700454 , -0.8647524 ,  0.12746282, -2.109626  , -5.8655405 ],\n",
              "       [ 0.91129225, -1.170349  , -3.744451  , -6.723289  , -7.957592  ]],\n",
              "      dtype=float32), label_ids=array([[0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.2642548382282257, 'test_accuracy': 0.6954751131221719, 'test_f1': 0.6912560441433127, 'test_confusion': '[[169 108   2   0   0]\\n [ 81 480  59  12   1]\\n [  6 105 190  85   3]\\n [  0   6  25 391  88]\\n [  0   0   4  88 307]]', 'test_runtime': 11.9404, 'test_samples_per_second': 185.086, 'test_steps_per_second': 23.199})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, sents_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNAOageTZudp",
        "outputId": "51031d8d-7172-42bf-b383-d1ab3a694e88"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(2), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(1), tensor(3), tensor(4), tensor(4), tensor(2), tensor(2), tensor(3), tensor(1), tensor(4), tensor(3), tensor(4), tensor(1), tensor(2), tensor(3), tensor(0), tensor(3), tensor(4), tensor(3), tensor(2), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(2), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(2), tensor(4), tensor(4), tensor(4), tensor(3), tensor(2), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(1), tensor(3), tensor(1), tensor(3), tensor(3), tensor(4), tensor(2), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(2), tensor(4), tensor(3), tensor(2), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(1), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(1), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(1), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(0), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(1), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(1), tensor(3), tensor(3), tensor(2), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(2), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(2), tensor(4), tensor(3), tensor(1), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(4), tensor(3), tensor(4), tensor(2), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(2), tensor(3), tensor(1), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(1), tensor(4), tensor(3), tensor(2), tensor(2), tensor(4), tensor(3), tensor(2), tensor(4), tensor(3), tensor(1), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(1), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(2), tensor(1), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(1), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(0), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(2), tensor(2), tensor(2), tensor(3), tensor(4), tensor(2), tensor(4), tensor(4), tensor(4), tensor(4), tensor(2), tensor(3), tensor(4), tensor(1), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(2), tensor(4), tensor(3), tensor(2), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(1), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(0), tensor(3), tensor(3), tensor(3), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(1), tensor(2), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(1), tensor(3), tensor(1), tensor(4), tensor(3), tensor(4), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(2), tensor(4), tensor(3), tensor(3), tensor(3), tensor(1), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(1), tensor(2), tensor(2), tensor(4), tensor(4), tensor(3), tensor(3), tensor(1), tensor(2), tensor(2), tensor(3), tensor(4), tensor(4), tensor(3), tensor(2), tensor(1), tensor(2), tensor(3), tensor(3), tensor(2), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(4), tensor(2), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(1), tensor(3), tensor(3), tensor(1), tensor(2), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(1), tensor(3), tensor(2), tensor(3), tensor(3), tensor(1), tensor(4), tensor(2), tensor(0), tensor(4), tensor(1), tensor(1), tensor(1), tensor(2), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(1), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(2), tensor(3), tensor(1), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(2), tensor(4), tensor(4), tensor(4), tensor(4), tensor(1), tensor(4), tensor(3), tensor(4), tensor(1), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(0), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(0), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(1), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(2), tensor(4), tensor(2), tensor(2), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(2), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(1), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(0), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(2), tensor(2), tensor(2), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(2), tensor(3), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(2), tensor(1), tensor(2), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(3), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(1), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(1), tensor(3), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(2), tensor(1), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(1), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(1), tensor(3), tensor(2), tensor(1), tensor(4), tensor(3), tensor(4), tensor(4), tensor(2), tensor(4), tensor(1), tensor(3), tensor(4), tensor(1), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(1), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(1), tensor(2), tensor(4), tensor(2), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(4), tensor(4), tensor(1), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(1), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(2), tensor(4), tensor(4), tensor(3), tensor(0), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(1), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(1), tensor(0), tensor(3), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(2), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(3), tensor(3), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(3), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(0), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(3), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(3), tensor(1), tensor(4), tensor(1), tensor(3), tensor(0), tensor(2), tensor(3), tensor(3), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(3), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(3), tensor(0), tensor(1), tensor(0), tensor(3), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(3), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(0), tensor(1), tensor(3), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(2), tensor(2), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(3), tensor(1), tensor(0), tensor(3), tensor(2), tensor(2), tensor(2), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(3), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(3), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(3), tensor(1), tensor(2), tensor(1), tensor(0), tensor(0), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(0), tensor(2), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(2), tensor(2), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(3), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(2), tensor(4), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(3), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(3), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(3), tensor(1), tensor(2), tensor(2), tensor(1), tensor(2), tensor(2), tensor(2), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(3), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(3), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(2), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(3), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(3), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(3), tensor(1), tensor(3), tensor(0), tensor(3), tensor(1), tensor(0), tensor(1), tensor(0), tensor(3), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(2), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(3), tensor(3), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(2), tensor(2), tensor(2), tensor(1), tensor(3), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(3), tensor(2), tensor(1), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(2), tensor(0), tensor(4), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(4), tensor(1), tensor(2), tensor(2), tensor(0), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(3), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(3), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(4), tensor(2), tensor(1), tensor(2), tensor(1), tensor(3), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(3), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(3), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(3), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(2), tensor(0), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(0), tensor(2), tensor(0), tensor(2), tensor(3), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(3), tensor(2), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(3), tensor(3), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(3), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(3), tensor(4), tensor(3), tensor(2), tensor(0)] [tensor(2), tensor(3), tensor(4), tensor(2), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(2), tensor(3), tensor(2), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(1), tensor(3), tensor(4), tensor(4), tensor(2), tensor(3), tensor(3), tensor(1), tensor(3), tensor(3), tensor(3), tensor(1), tensor(2), tensor(4), tensor(0), tensor(4), tensor(4), tensor(3), tensor(2), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(2), tensor(1), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(2), tensor(4), tensor(4), tensor(4), tensor(3), tensor(1), tensor(4), tensor(3), tensor(3), tensor(2), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(1), tensor(3), tensor(0), tensor(3), tensor(3), tensor(4), tensor(2), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(2), tensor(2), tensor(4), tensor(3), tensor(2), tensor(4), tensor(3), tensor(1), tensor(3), tensor(2), tensor(3), tensor(4), tensor(4), tensor(4), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(1), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(1), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(1), tensor(4), tensor(4), tensor(4), tensor(1), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(3), tensor(4), tensor(4), tensor(0), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(2), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(2), tensor(4), tensor(3), tensor(2), tensor(1), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(1), tensor(4), tensor(4), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(4), tensor(3), tensor(1), tensor(2), tensor(4), tensor(3), tensor(3), tensor(2), tensor(3), tensor(4), tensor(2), tensor(3), tensor(3), tensor(4), tensor(2), tensor(4), tensor(3), tensor(2), tensor(2), tensor(3), tensor(3), tensor(1), tensor(2), tensor(3), tensor(4), tensor(4), tensor(4), tensor(2), tensor(4), tensor(3), tensor(4), tensor(3), tensor(2), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(2), tensor(4), tensor(3), tensor(3), tensor(1), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(1), tensor(4), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(2), tensor(4), tensor(4), tensor(2), tensor(2), tensor(2), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(1), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(1), tensor(2), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(2), tensor(3), tensor(3), tensor(4), tensor(2), tensor(4), tensor(4), tensor(4), tensor(1), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(2), tensor(4), tensor(1), tensor(3), tensor(2), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(1), tensor(3), tensor(4), tensor(4), tensor(4), tensor(2), tensor(3), tensor(2), tensor(3), tensor(2), tensor(3), tensor(4), tensor(1), tensor(4), tensor(4), tensor(4), tensor(4), tensor(2), tensor(2), tensor(4), tensor(1), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(2), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(0), tensor(3), tensor(3), tensor(3), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(2), tensor(2), tensor(2), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(2), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(1), tensor(3), tensor(0), tensor(4), tensor(2), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(3), tensor(3), tensor(1), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(2), tensor(2), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(1), tensor(2), tensor(2), tensor(4), tensor(3), tensor(3), tensor(3), tensor(1), tensor(2), tensor(2), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(1), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(4), tensor(2), tensor(3), tensor(4), tensor(3), tensor(2), tensor(4), tensor(3), tensor(2), tensor(2), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(2), tensor(4), tensor(2), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(2), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(1), tensor(2), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(1), tensor(3), tensor(2), tensor(3), tensor(3), tensor(1), tensor(4), tensor(3), tensor(0), tensor(4), tensor(1), tensor(0), tensor(1), tensor(2), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(1), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(2), tensor(4), tensor(1), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(4), tensor(3), tensor(4), tensor(1), tensor(4), tensor(3), tensor(4), tensor(1), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(2), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(0), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(0), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(2), tensor(2), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(1), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(2), tensor(4), tensor(2), tensor(2), tensor(4), tensor(2), tensor(3), tensor(3), tensor(4), tensor(2), tensor(4), tensor(3), tensor(2), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(2), tensor(4), tensor(2), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(2), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(0), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(1), tensor(2), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(4), tensor(2), tensor(1), tensor(2), tensor(2), tensor(3), tensor(4), tensor(2), tensor(4), tensor(2), tensor(2), tensor(2), tensor(2), tensor(3), tensor(2), tensor(3), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(1), tensor(2), tensor(4), tensor(4), tensor(4), tensor(4), tensor(4), tensor(2), tensor(3), tensor(4), tensor(2), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(2), tensor(3), tensor(1), tensor(3), tensor(4), tensor(4), tensor(4), tensor(3), tensor(2), tensor(4), tensor(4), tensor(4), tensor(2), tensor(3), tensor(3), tensor(2), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(2), tensor(1), tensor(2), tensor(3), tensor(4), tensor(4), tensor(4), tensor(1), tensor(4), tensor(4), tensor(4), tensor(4), tensor(3), tensor(4), tensor(0), tensor(3), tensor(2), tensor(0), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(4), tensor(2), tensor(2), tensor(4), tensor(1), tensor(3), tensor(3), tensor(4), tensor(3), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(1), tensor(2), tensor(3), tensor(2), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(1), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(2), tensor(4), tensor(4), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(3), tensor(2), tensor(1), tensor(1), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(1), tensor(2), tensor(4), tensor(2), tensor(0), tensor(4), tensor(3), tensor(4), tensor(4), tensor(4), tensor(1), tensor(2), tensor(2), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(3), tensor(2), tensor(3), tensor(4), tensor(3), tensor(2), tensor(3), tensor(4), tensor(3), tensor(3), tensor(3), tensor(4), tensor(3), tensor(3), tensor(4), tensor(3), tensor(2), tensor(1), tensor(1), tensor(4), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(3), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(2), tensor(2), tensor(3), tensor(4), tensor(3), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(0), tensor(4), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(4), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(0), tensor(2), tensor(2), tensor(1), tensor(2), tensor(0), tensor(3), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(2), tensor(0), tensor(3), tensor(1), tensor(4), tensor(1), tensor(3), tensor(0), tensor(1), tensor(2), tensor(3), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(3), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(4), tensor(2), tensor(1), tensor(3), tensor(2), tensor(2), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(2), tensor(1), tensor(2), tensor(1), tensor(1), tensor(3), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(3), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(2), tensor(0), tensor(2), tensor(2), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(3), tensor(1), tensor(0), tensor(3), tensor(2), tensor(3), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(2), tensor(1), tensor(0), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(3), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(3), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(3), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(3), tensor(1), tensor(2), tensor(1), tensor(0), tensor(0), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(0), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(3), tensor(2), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(0), tensor(2), tensor(2), tensor(1), tensor(4), tensor(1), tensor(2), tensor(0), tensor(2), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(0), tensor(2), tensor(1), tensor(1), tensor(0), tensor(2), tensor(2), tensor(2), tensor(1), tensor(2), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(4), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(2), tensor(0), tensor(0), tensor(3), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(0), tensor(2), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(3), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(3), tensor(1), tensor(2), tensor(2), tensor(0), tensor(3), tensor(2), tensor(2), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(3), tensor(0), tensor(0), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(3), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(2), tensor(1), tensor(0), tensor(0), tensor(0), tensor(3), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(0), tensor(1), tensor(1), tensor(3), tensor(1), tensor(4), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(3), tensor(1), tensor(2), tensor(1), tensor(3), tensor(1), tensor(0), tensor(2), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(2), tensor(2), tensor(2), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(3), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(2), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(0), tensor(2), tensor(1), tensor(2), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(1), tensor(3), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(3), tensor(1), tensor(1), tensor(3), tensor(2), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(4), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(3), tensor(1), tensor(2), tensor(1), tensor(0), tensor(2), tensor(0), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(2), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(1), tensor(4), tensor(1), tensor(1), tensor(3), tensor(1), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(3), tensor(1), tensor(2), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(3), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(3), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(3), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(2), tensor(2), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(2), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(3), tensor(2), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(2), tensor(1), tensor(1), tensor(2), tensor(0), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(3), tensor(3), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(0), tensor(0), tensor(2), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(2), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(3), tensor(4), tensor(4), tensor(3), tensor(0)]\n",
            "[[169 108   2   0   0]\n",
            " [ 81 480  59  12   1]\n",
            " [  6 105 190  85   3]\n",
            " [  0   6  25 391  88]\n",
            " [  0   0   4  88 307]]\n",
            "Accuracy: 0.6954751131221719\n",
            "f1: 0.6912560441433127\n",
            "precision: [0.66015625 0.68669528 0.67857143 0.67881944 0.76942356]\n",
            "recall: [0.60573477 0.75829384 0.48843188 0.76666667 0.76942356]\n",
            "fscore: [0.6317757  0.72072072 0.56801196 0.72007366 0.76942356]\n",
            "support: [279 633 389 510 399]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, distilbert_tokenizer, \"I don't think that the movie, which my father told me about last night when we returned home, was amazing\", 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "6pnes3bnXFRC",
        "outputId": "6700460e-d023-434a-badc-a6c8cf82bc31"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, distilbert_tokenizer, \"I think that the movie, which my father told me about last night when we returned home, was amazing\", 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "t8yJwN7L4ZmL",
        "outputId": "51d7dd2d-9984-42c3-a20a-d7c62a2a017a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'very positive'"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, distilbert_tokenizer, \"The chicken crossed the road, it got struck by a car, but it survived and it's rich \", 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "k4WEgzm_4pW7",
        "outputId": "ad916e0e-98dd-4ada-b01a-8a3225b6ce08"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'positive'"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    }
  ]
}